<!doctype html>
<html lang="en">
<head>
	<title>Your Project Name</title>
	<meta content="Your" property="og:title" />
	<meta content="Your Project Name" name="twitter:title"/>
	<meta content="Your project about your cool topic described right here." name="description"/>
	<meta content="Your project about your cool topic described right here." property="og:description"/>
	<meta content="Your project about your cool topic described right here." name="twitter:description"/>
	<meta content="website" property="og:type"/>
	<meta content="summary" name="twitter:card"/>
	<meta content="width=device-width,initial-scale=1" name="viewport"/>
	<!-- bootstrap for mobile-friendly layout -->
	<link crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css"
	      integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" rel="stylesheet">
	<script crossorigin="anonymous"
	        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
	        src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js"></script>
	<script crossorigin="anonymous"
	        integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct"
	        src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
	<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
	<link href="style.css" rel="stylesheet">
</head>
<body class="nd-docs">
<div class="nd-pageheader">
	<div class="container">
		<h1 class="lead">
			<nobr class="widenobr">Fine-tuning BERT</nobr>
			<nobr class="widenobr">For CS 7150</nobr>
		</h1>
	</div>
</div><!-- end nd-pageheader -->

<div class="container">
	<div class="row">
		<div class="col justify-content-center text-justify">
			<h2>An Analysis of BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</h2>
			<p>
				<strong>Abstract: </strong>
				We introduce a new language representation model called <strong>BERT</strong>,
				which stands for <strong>B</strong>idirectional <strong>E</strong>ncoder
				<strong>R</strong>epresentations from <strong>T</strong>ransformers. Unlike
				recent language representation models <a href="#dcwr">[2]</a> <a href="#ilugpt">[3]</a>,
				BERT is designed to pretrain deep bidirectional representations from unlabeled text
				by jointly conditioning on both left and right context in all layers. As a result,
				the pre-trained BERT model can be finetuned with just one additional output layer to
				create state-of-the-art models for a wide range of tasks, such as question answering
				and language inference, without substantial taskspecific architecture modifications.
				<br>
				BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art
				results on eleven natural language processing tasks, including pushing the GLUE score
				to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute
				improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute
				improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).
			</p>
		</div>
	</div>
	<div class="row">
		<div class="col">
			<h2>Literature Review</h2>
			<p>
				There is a long history of pre-training general language representations
			</p>
			<h6><b>Feature-based</b></h6>
			<ul>
				<li><label>Word embeddings:&nbsp;</label>Hinton&apos;09, Turian&apos;10, Le&apos;14,
					Kiros&apos;15, Peters&apos;17, Lee&apos;18</li>
				<li><label>Sentence representations:&nbsp;</label>Mikolov&apos;13, Kiros&apos;15,
					Hill&apos;16, Malamud&apos;16</li>
			</ul>
			<h6><b>Fine-tuning</b></h6>
			<ul>
				<li><label>Pre-trained using unsupervised data:&nbsp;</label>Dai&apos;15, Ruder&apos;18,
					Radford&apos;18 {OpenAI GPT}</li>
				<li><label>Transfer learning from supervised data:&nbsp;</label>Deng&apos;09,
					Yosinski&apos;14, Conneau&apos;17, McCann&apos;17</li>
			</ul>
			<b>Introduced in 2018, the paper has already been cited over 85000 times!</b>

			<h2>Biography</h2>
			<div class="img-div">
				<div class="img-div-sub">
					<img src="./imgs/Jacob.jpeg" width="200" height="200">
					<p>
						C: ? H: ? <br>
						Software Engineer @ Google <br>
						MS CS @ University of Maryland
					</p>
				</div>
				<div class="img-div-sub">
					<img src="./imgs/Cheng.jpeg" width="200" height="200">
					<p>
						C: 98,000+ H: 47 <br>
						Research scientist @ Google <br>
						PHD CS @ UIUC
					</p>
				</div>
				<div class="img-div-sub">
					<img src="./imgs/Kenton.jpeg" width="200" height="200">
					<p>
						C: 109,000+ H: 32 <br>
						Research scientist @ Google <br>
						PHD CS @ University of Washington
					</p>
				</div>
				<div class="img-div-sub">
					<img src="./imgs/Kristina.jpeg" width="200" height="200">
					<p>
						C: 103,000+ H: 49 <br>
						Research scientist @ Google <br>
						PHD CS @ Stanford University
					</p>
				</div>
			</div>

			<h2>Diagrams</h2>
			<div class="dig-div">
				<img src="./imgs/Dig1.png">
				<br><br><br><br>
				<img src="./imgs/Dig2.png">
			</div>

			<h2>Social Impact</h2>
			<p>
				<h6><b>Positive Impact</b></h6>
				<ul>
					<li><b>Advancements in Chatbots and Virtual Assistants</b>:&nbsp;BERT has contributed
						to the development of more sophisticated and context-aware chatbots and virtual
						assistants, enabling more natural and effective human-computer interactions.</li>
					<li><b>Enhanced Search Engine Results</b>:&nbsp;Search engines like Google use BERT to better
						understand the context and nuances of search queries, leading to more accurate
						and relevant search results. This has improved the overall search experience
						for users.</li>
				</ul>
				<h6><b>Negative Impact</b></h6>
				<ul>
					<li><b>Challenges in Bias and Fairness</b>:&nbsp;The training data used for models like BERT
						may contain biases present in the language data. As a result, these biases
						can be perpetuated or even amplified in the model's output, potentially leading
						to biased or unfair results. Addressing bias and ensuring fairness in language
						models is an ongoing challenge.</li>
					<li><b>Privacy Concerns</b>:&nbsp;The use of large language models raises privacy concerns,
						especially when handling sensitive information. There are risks associated
						with the unintentional generation of sensitive content or the extraction of
						sensitive information from the training data.</li>
					<li><b>Resource Intensiveness</b>:&nbsp;Training large models like BERT requires significant
						computational resources, which can contribute to environmental concerns.
						Additionally, deploying and running such models in production may require
						substantial computational power.</li>
				</ul>
			</p>

			<h2>Industry Applications</h2>
			<p>
				<ul>
					<li><b>Chatbots & Virtual Assistants:</b> Better at understanding NLP queries.</li>
					<li><b>Text summarization:</b> For generating meaningful summaries of longer texts.</li>
					<li><b>E-commerce:</b> can enhance the search and recommendation systems in e-commerce
						platforms by better understanding user queries.</li>
					<li><b>Financial Analysis:</b> BERT is used in financial applications for tasks such as
						sentiment analysis of financial news. </li>
					<li><b>Search Engines:</b> Improves understanding of user queries.</li>
					<li><b>Medical & Healthcare:</b> BERT has been applied to medical and healthcare domains
						for tasks such as clinical text analysis, medical record summarization, and
						information extraction from medical literature.</li>
					<li><b>Content generation:</b> BERT can be used in content generation tools to produce
						high-quality and contextually relevant text.</li>
					<li><b>Human Resources:</b> Assists in the analysis of resumes and job descriptions.</li>
				</ul>
			</p>

			<h2>Follow-on Research</h2>
			<p>

			</p>

			<h2>Peer-Review</h2>
			<ul>
				<li></li>
				<li></li>
				<li></li>
			</ul>

			<h3>References</h3>
			<p>
				<a id="bert">[1]</a>
				<a href="https://arxiv.org/pdf/1810.04805.pdf">
					Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova.
					<em>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.</em>
				</a>
				arXiv.org (2018, October 11).
			</p>
			<p>
				<a id="dcwr">[2]</a>
				<a href="https://arxiv.org/pdf/1802.05365.pdf">
					Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner.
					Christopher Clark, Kenton Lee, Luke Zettlemoyer.
					<em>Deep contextualized word representations.</em>
				</a>
				arXiv.org (2018, March 22).
			</p>
			<p>
				<a id="ilugpt">[3]</a>
				<a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">
					Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever.
					<em>Improving Language Understanding by Generative Pre-Training.</em>
				</a>
				OpenAI (2018, June 11).
			</p>

			<h2>Team Members</h2>
			<p>Chandra Teja Kommineni, Debajyoti Chakraborty, Ekam Chahal</p>
		</div><!--col-->
	</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
	<div class="row">
		<div class="col-6 col-md text-center">
			<a href="https://cs7150.baulab.info/">About CS 7150</a>
		</div>
	</div>
</footer>

</body>
<script>
	$(document).on('click', '.clickselect', function (ev) {
		var range = document.createRange();
		range.selectNodeContents(this);
		var sel = window.getSelection();
		sel.removeAllRanges();
		sel.addRange(range);
	});
	// Google analytics below.
	window.dataLayer = window.dataLayer || [];
</script>
</html>
