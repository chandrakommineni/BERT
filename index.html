<!doctype html>
<html lang="en">
<head>
	<title>Your Project Name</title>
	<meta content="Your" property="og:title" />
	<meta content="Your Project Name" name="twitter:title"/>
	<meta content="Your project about your cool topic described right here." name="description"/>
	<meta content="Your project about your cool topic described right here." property="og:description"/>
	<meta content="Your project about your cool topic described right here." name="twitter:description"/>
	<meta content="website" property="og:type"/>
	<meta content="summary" name="twitter:card"/>
	<meta content="width=device-width,initial-scale=1" name="viewport"/>
	<!-- bootstrap for mobile-friendly layout -->
	<link crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css"
	      integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" rel="stylesheet">
	<script crossorigin="anonymous"
	        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
	        src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js"></script>
	<script crossorigin="anonymous"
	        integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct"
	        src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
	<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
	<link href="style.css" rel="stylesheet">
</head>
<body class="nd-docs">
<div class="nd-pageheader">
	<div class="container">
		<h1 class="lead">
			<nobr class="widenobr">Fine-tuning BERT</nobr>
			<nobr class="widenobr">For CS 7150</nobr>
		</h1>
	</div>
</div><!-- end nd-pageheader -->

<div class="container">
	<div class="row">
		<div class="col justify-content-center text-justify">
			<h2>An Analysis of BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</h2>
			<p>
				<strong>Abstract: </strong>
				We introduce a new language representation model called <strong>BERT</strong>,
				which stands for <strong>B</strong>idirectional <strong>E</strong>ncoder
				<strong>R</strong>epresentations from <strong>T</strong>ransformers. Unlike
				recent language representation models <a href="#dcwr">[2]</a> <a href="#ilugpt">[3]</a>,
				BERT is designed to pretrain deep bidirectional representations from unlabeled text
				by jointly conditioning on both left and right context in all layers. As a result,
				the pre-trained BERT model can be finetuned with just one additional output layer to
				create state-of-the-art models for a wide range of tasks, such as question answering
				and language inference, without substantial taskspecific architecture modifications.
				<br>
				BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art
				results on eleven natural language processing tasks, including pushing the GLUE score
				to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute
				improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute
				improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).
			</p>
		</div>
	</div>
	<div class="row">
		<div class="col">
			<h2>Literature Review</h2>
			<p>
				There is a long history of pre-training general language representations
			</p>
			<h6><b>Feature-based</b></h6>
			<ul>
				<li><label>Word embeddings:&nbsp;</label>Hinton&apos;09, Turian&apos;10, Le&apos;14,
					Kiros&apos;15, Peters&apos;17, Lee&apos;18</li>
				<li><label>Sentence representations:&nbsp;</label>Mikolov&apos;13, Kiros&apos;15,
					Hill&apos;16, Malamud&apos;16</li>
			</ul>
			<h6><b>Fine-tuning</b></h6>
			<ul>
				<li><label>Pre-trained using unsupervised data:&nbsp;</label>Dai&apos;15, Ruder&apos;18,
					Radford&apos;18 {OpenAI GPT}</li>
				<li><label>Transfer learning from supervised data:&nbsp;</label>Deng&apos;09,
					Yosinski&apos;14, Conneau&apos;17, McCann&apos;17</li>
			</ul>
			<b>Introduced in October 2018, the paper has already been cited over 85000 times!</b>

			<h2>Biography</h2>
			<div class="img-div">
				<div class="img-div-sub">
					<img src="./imgs/Jacob.jpeg" width="200" height="200">
					<p>
						C: ? H: ? <br>
						Software Engineer @ Google <br>
						MS CS @ University of Maryland
					</p>
				</div>
				<div class="img-div-sub">
					<img src="./imgs/Cheng.jpeg" width="200" height="200">
					<p>
						C: 98,000+ H: 47 <br>
						Research scientist @ Google <br>
						PHD CS @ UIUC
					</p>
				</div>
				<div class="img-div-sub">
					<img src="./imgs/Kenton.jpeg" width="200" height="200">
					<p>
						C: 109,000+ H: 32 <br>
						Research scientist @ Google <br>
						PHD CS @ University of Washington
					</p>
				</div>
				<div class="img-div-sub">
					<img src="./imgs/Kristina.jpeg" width="200" height="200">
					<p>
						C: 103,000+ H: 49 <br>
						Research scientist @ Google <br>
						PHD CS @ Stanford University
					</p>
				</div>
			</div>

			<h2>Diagrams</h2>
			<div class="dig-div">
				<img src="imgs/Dig1.png">
				<br><br><br><br>
				<img src="imgs/Dig2.png">
			</div>

			<h2>Social Impact</h2>
			<p>
				<h6><b>Positive Impact</b></h6>
				<ul>
					<li><b>Advancements in Chatbots and Virtual Assistants</b>:&nbsp;BERT has contributed
						to the development of more sophisticated and context-aware chatbots and virtual
						assistants, enabling more natural and effective human-computer interactions.</li>
					<li><b>Enhanced Search Engine Results</b>:&nbsp;Search engines like Google use BERT to better
						understand the context and nuances of search queries, leading to more accurate
						and relevant search results. This has improved the overall search experience
						for users.</li>
				</ul>
				<h6><b>Negative Impact</b></h6>
				<ul>
					<li><b>Challenges in Bias and Fairness</b>:&nbsp;The training data used for models like BERT
						may contain biases present in the language data. As a result, these biases
						can be perpetuated or even amplified in the model's output, potentially leading
						to biased or unfair results. Addressing bias and ensuring fairness in language
						models is an ongoing challenge.</li>
					<li><b>Privacy Concerns</b>:&nbsp;The use of large language models raises privacy concerns,
						especially when handling sensitive information. There are risks associated
						with the unintentional generation of sensitive content or the extraction of
						sensitive information from the training data.</li>
					<li><b>Resource Intensiveness</b>:&nbsp;Training large models like BERT requires significant
						computational resources, which can contribute to environmental concerns.
						Additionally, deploying and running such models in production may require
						substantial computational power.</li>
				</ul>
			</p>

			<h2>Industry Applications</h2>
			<p>
				<ul>
					<li><b>Chatbots & Virtual Assistants:</b> Better at understanding NLP queries.</li>
					<li><b>Text summarization:</b> For generating meaningful summaries of longer texts.</li>
					<li><b>E-commerce:</b> can enhance the search and recommendation systems in e-commerce
						platforms by better understanding user queries.</li>
					<li><b>Financial Analysis:</b> BERT is used in financial applications for tasks such as
						sentiment analysis of financial news. </li>
					<li><b>Search Engines:</b> Improves understanding of user queries.</li>
					<li><b>Medical & Healthcare:</b> BERT has been applied to medical and healthcare domains
						for tasks such as clinical text analysis, medical record summarization, and
						information extraction from medical literature.</li>
					<li><b>Content generation:</b> BERT can be used in content generation tools to produce
						high-quality and contextually relevant text.</li>
					<li><b>Human Resources:</b> Assists in the analysis of resumes and job descriptions.</li>
				</ul>
			</p>

			<h2>Follow-on Research</h2>
			<p><i><b>Note:</b> The following papers were released within one year of BERT (May 2019 - Oct 2019)</i></p>
			<ul>
				<li><b>ERNIE <a href="#ernie">[4]</a> :</b>
					Incorporates information-rich knowledge graphs during pre-training to enhance language
					representation with lexical, syntactic, and knowledge information simultaneously.
				</li>
				<li><b>XLNet <a href="#xlnet">[5]</a> :</b>
					Enables bidirectional context learning through permutation-based training and addresses the
					"pretrain-finetune discrepancy" (dependency between masked tokens) with its autoregressive formulation.
				</li>
				<li><b>RoBERTa <a href="#roberta">[6]</a> :</b>
					Improves over BERT by conducting a meticulous replication study, optimizes key
					hyperparameters, and demonstrates that BERT was significantly undertrained.
				</li>
				<li><b>ALBERT <a href="#albert">[7]</a> :</b>
					Introduces two parameter-reduction techniques to enhance memory efficiency and training speed
					with scalability, and incorporates a self-supervised loss for inter-sentence coherence modeling.
				</li>
				<li><b>DistilBERT <a href="#distilbert">[8]</a> :</b>
					Uses knowledge distillation to compress BERT during pre-training, and shows that it is
					possible to reduce the size of a BERT model by 40%, while retaining 97% of its language
					understanding capabilities and being 60% faster.
				</li>
				<li><b>T5 <a href="#t5">[9]</a> :</b>
					Introduces a unified text-to-text framework for transfer learning in NLP, and systematically
					explores various pre-training objectives, architectures, and transfer learning approaches.
				</li>
			</ul>

			<h2>Peer-Review</h2>
			<ul>
				<li></li>
				<li></li>
				<li></li>
			</ul>

			<h3>References</h3>
			<p>
				<a id="bert">[1]</a>
				<a href="https://arxiv.org/pdf/1810.04805.pdf">
					Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova.
					<em>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.</em>
				</a>
				arXiv.org (2018, October 11).
			</p>
			<p>
				<a id="dcwr">[2]</a>
				<a href="https://arxiv.org/pdf/1802.05365.pdf">
					Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner.
					Christopher Clark, Kenton Lee, Luke Zettlemoyer.
					<em>Deep contextualized word representations.</em>
				</a>
				arXiv.org (2018, March 22).
			</p>
			<p>
				<a id="ilugpt">[3]</a>
				<a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">
					Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever.
					<em>Improving Language Understanding by Generative Pre-Training.</em>
				</a>
				OpenAI (2018, June 11).
			</p>
			<p>
				<a id="ernie">[4]</a>
				<a href="https://arxiv.org/pdf/1905.07129.pdf">
					Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, Qun Liu.
					<em>ERNIE: Enhanced Language Representation with Informative Entities.</em>
				</a>
				arXiv.org (2019, May 17).
			</p>
			<p>
				<a id="xlnet">[5]</a>
				<a href="https://arxiv.org/pdf/1906.08237.pdf">
					Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.
					<em>XLNet: Generalized Autoregressive Pretraining for Language Understanding.</em>
				</a>
				arXiv.org (2019, June 19).
			</p>
			<p>
				<a id="roberta">[6]</a>
				<a href="https://arxiv.org/pdf/1907.11692.pdf">
					Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.
					<em>RoBERTa: A Robustly Optimized BERT Pretraining Approach.</em>
				</a>
				arXiv.org (2019, July 26).
			</p>
			<p>
				<a id="albert">[7]</a>
				<a href="https://arxiv.org/pdf/1909.11942.pdf">
					Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.
					<em>ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.</em>
				</a>
				arXiv.org (2019, September 26).
			</p>
			<p>
				<a id="distilbert">[8]</a>
				<a href="https://arxiv.org/pdf/1910.01108.pdf">
					Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf.
					<em>DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.</em>
				</a>
				arXiv.org (2019, October 2).
			</p>
			<p>
				<a id="t5">[9]</a>
				<a href="https://arxiv.org/pdf/1910.10683.pdf">
					Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu.
					<em>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.</em>
				</a>
				arXiv.org (2019, October 23).
			</p>

			<h2>Team Members</h2>
			<p>Chandra Teja Kommineni, Ekam Chahal, Debajyoti Chakraborty</p>
		</div><!--col-->
	</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
	<div class="row">
		<div class="col-6 col-md text-center">
			<a href="https://cs7150.baulab.info/">About CS 7150</a>
		</div>
	</div>
</footer>

</body>
<script>
	$(document).on('click', '.clickselect', function (ev) {
		var range = document.createRange();
		range.selectNodeContents(this);
		var sel = window.getSelection();
		sel.removeAllRanges();
		sel.addRange(range);
	});
	// Google analytics below.
	window.dataLayer = window.dataLayer || [];
</script>
</html>
