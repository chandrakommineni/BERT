<!doctype html>
<html lang="en">
<head>
	<title>Your Project Name</title>
	<meta content="Your" property="og:title" />
	<meta content="Your Project Name" name="twitter:title"/>
	<meta content="Your project about your cool topic described right here." name="description"/>
	<meta content="Your project about your cool topic described right here." property="og:description"/>
	<meta content="Your project about your cool topic described right here." name="twitter:description"/>
	<meta content="website" property="og:type"/>
	<meta content="summary" name="twitter:card"/>
	<meta content="width=device-width,initial-scale=1" name="viewport"/>
	<!-- bootstrap for mobile-friendly layout -->
	<link crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css"
	      integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" rel="stylesheet">
	<script crossorigin="anonymous"
	        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
	        src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js"></script>
	<script crossorigin="anonymous"
	        integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct"
	        src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
	<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
	<link href="style.css" rel="stylesheet">
</head>
<body class="nd-docs">
<div class="nd-pageheader">
	<div class="container">
		<h1 class="lead">
			<nobr class="widenobr">Fine-tuning BERT</nobr>
			<nobr class="widenobr">For CS 7150</nobr>
		</h1>
	</div>
</div><!-- end nd-pageheader -->

<div class="container">
	<div class="row">
		<div class="col justify-content-center text-center">
			<h2>An Analysis of BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</h2>
			<p>
				<strong>Abstract: </strong>
				We introduce a new language representation model called <strong>BERT</strong>,
				which stands for <strong>B</strong>idirectional <strong>E</strong>ncoder
				<strong>R</strong>epresentations from <strong>T</strong>ransformers. Unlike
				recent language representation models (Peters et al., 2018a; Radford et al., 2018),
				BERT is designed to pretrain deep bidirectional representations from unlabeled text
				by jointly conditioning on both left and right context in all layers. As a result,
				the pre-trained BERT model can be finetuned with just one additional output layer to
				create state-of-the-art models for a wide range of tasks, such as question answering
				and language inference, without substantial taskspecific architecture modifications.

				BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art
				results on eleven natural language processing tasks, including pushing the GLUE score
				to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute
				improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute
				improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).
			</p>
		</div>
	</div>
	<div class="row">
		<div class="col">

			<h2>Literature Review</h2>
			<p>

			</p>

			<h2>Biography</h2>
			<p>

			</p>

			<h2>Social Impact</h2>
			<p>

			</p>

			<h2>Industry Applications</h2>
			<p>

			</p>

			<h2>Follow-on Research</h2>
			<p>

			</p>

			<h2>Peer-Review</h2>
			<p>

			</p>

			<h3>References</h3>
			<p>
				<a id="bert">[1]</a>
				<a href="https://arxiv.org/pdf/1810.04805.pdf">
					Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova.
					<em>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.</em>
				</a>
				arXiv.org (2018, October 11).
			</p>
			<p>
				<a id="dcwr">[2]</a>
				<a href="https://arxiv.org/pdf/1802.05365.pdf">
					Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner.
					Christopher Clark, Kenton Lee, Luke Zettlemoyer.
					<em>Deep contextualized word representations.</em>
				</a>
				arXiv.org (2018, March 22).
			</p>
			<p>
				<a id="ilugpt">[3]</a>
				<a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">
					Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever.
					<em>Improving Language Understanding by Generative Pre-Training.</em>
				</a>
				OpenAI (2018, June 11).
			</p>

			<h2>Team Members</h2>

			<p>Make sure to list here is who is on the team.</p>
		</div><!--col-->
	</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
	<div class="row">
		<div class="col-6 col-md text-center">
			<a href="https://cs7150.baulab.info/">About CS 7150</a>
		</div>
	</div>
</footer>

</body>
<script>
	$(document).on('click', '.clickselect', function (ev) {
		var range = document.createRange();
		range.selectNodeContents(this);
		var sel = window.getSelection();
		sel.removeAllRanges();
		sel.addRange(range);
	});
	// Google analytics below.
	window.dataLayer = window.dataLayer || [];
</script>
</html>
